{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact_manual\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import misc\n",
    "\n",
    "cmap = plt.cm.Pastel2\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means\n",
    "## Learn about K-means clustering by building the algorithm in numpy. Then use K-means to find groups of similar movies in the imdb5000 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In this tutorialWe will introduce **Unsupervised Learning** and **Clustering** to understand the problems **k-means** is meant to solve. We will learn how k-means works by building the algorithm ourselves in numpy. At first, we will test and evaluate our implementation on simple synthetic data. Finally, we will use k-means to find groups of similar movies in the **imdb5000** dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "By now you should be familiar with **Supervised Learning** problems, which use a matrix of features, **X**, and a vector of labels, **y** (shown below). **Supervised Learning** can predict **y** given **X**.\n",
    "### Data for Supervised Learning\n",
    "\n",
    "![caption](supervised_data.svg)\n",
    "\n",
    "Supervised learning is very powerful, but for many projects labeled training data can be expensive and time consuming to obtain. \n",
    "\n",
    "### Data for Unsupervised Learning\n",
    "![caption](Unsupervised Data.svg)\n",
    "\n",
    "    \n",
    "Unlike supervised learning, **Unsupervised Learning** methods can extract meaningful insights from unlabeled training data. Unsupervised methods aim to reduce our data while still retaining the information present in the original data. There are two main forms of reduction in unsupervised learning:\n",
    "    \n",
    "    1. Dimensionality reduction - Reduce the number of dimensions.\n",
    "    2. Clustering - Reduce the number of samples.\n",
    "    \n",
    "In this lessonWe will be focused on the K-means algorithm which can be used for clustering.\n",
    "    \n",
    "![caption](unsupervised_learning.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "By now you should be familiar with **Supervised Learning** problems, where have access to a matrix of features **X** and a vector of labels **y** as shown below. The goal was to predict **y** given **X**.\n",
    "### Data for Supervised Learning\n",
    "\n",
    "![caption](supervised_data.svg)\n",
    "\n",
    "Supervised learning is very powerful, but for many projects labeled training data can be expensive and time consuming to obtain. This is where **Unsupervised Learning** methods come in.\n",
    "\n",
    "### Data for Unsupervised Learning\n",
    "![caption](Unsupervised Data.svg)\n",
    "\n",
    "    \n",
    "Using Unsupervised Learning methods.We can still extract meaningful insights from the data. Unsupervised methods aim to reduce our data while still retaining the information present in the original data. There are two main forms of reduction in unsupervised learning:\n",
    "    \n",
    "    1. Dimensionality reduction - Reduce the number of dimensions.\n",
    "    2. Clustering - Reduce the number of samples.\n",
    "    \n",
    "In this lessonWe will be focused on the K-means algorithm which can be used for clustering.\n",
    "    \n",
    "![caption](unsupervised_learning.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Clustering algorithms aim to reduce the number of samples in our data. They accomplish this reduction by grouping samples into subsets with other similar samples. These subsets of similar samples are called clusters. We can often treat each cluster as a single data point. Doing this can help us find structure in our data without getting lost in the details.\n",
    "\n",
    "In fact, we perform clustering unconsciously all the time. Consider a hypothetical High School which groups students into grades (9 - 12) based on their ages (14 - 18). By grouping (clustering) students in this way, the school can then produce content (coursework, social events, tests) for each grade, rather than for each individual student. They do this because they don't have the time or resources to produce content for each student individually.  \n",
    "\n",
    "![caption](clustering_dr.svg)\n",
    "\n",
    "### K-means reducing a dataset of 250 points to 3 clusters\n",
    "![caption](kmeans.gif)\n",
    "\n",
    "Consciously and non-exhaustively, clustering algorithms like K-means are used for:\n",
    "\n",
    "- **Business** - to group similar customers together then target them with personalized ads or coupons.\n",
    "- **Search** - to group similar documents or images together to aid in retrieval.\n",
    "- **Data Compression** - to group similar colors together to representing an image with 16 or 32 colors.\n",
    "- **Biology** - delimiting species together by morphology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Clustering algorithms aim to reduce the number of samples in our data. They accomplish this reduction by grouping samples into subsets with other similar samples. These subsets of similar samples are called clusters.We can often treat each cluster as a single data point. Doing this can help us find structure in our data without getting lost in the details.\n",
    "\n",
    "In fact,We perform clustering unconsciously all the time. Consider a (simplified version of) a High School. High Schools group students into grades (9 - 12) based on their ages (14 - 18). For the most part, they then treat each grade as a single data point and produce content (coursework, social events, tests) for each grade, rather than for each student. They do this because they don't have the time or resources to produce content for each student individually.  \n",
    "\n",
    "![caption](clustering_dr.svg)\n",
    "\n",
    "### K-means reducing a dataset of 250 points to 3 clusters\n",
    "![caption](kmeans.gif)\n",
    "\n",
    "Consciously and non-exhaustively, clustering algorithms like K-means are used in:\n",
    "\n",
    "- **Business** - to group similar customers together then target them with personalized ads or coupons.\n",
    "- **Search** - to group similar documents or images together to aid in retrieval.\n",
    "- **Data Compression** - to group similar colors together to representing an image with 16 or 32 colors.\n",
    "- **Biology** - to group genes together that have similar functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Warning\n",
    "By now I'm sure you think clustering is aIsome and you're ready to use it to find structure in all of your cheap, easy to obtain, unlabeled data. Before you go ahead and ```from sklearn.cluster import kmeans```, you need to know that k-means and unsupervised learning in general can be difficult to evaluate. Because of this, it is important to know what the algorithms are doing so you can decide if they will work Ill on your dataset and so you can fix things if they go wrong. So let's take a peek under the hood.\n",
    "\n",
    "# K-means\n",
    "K-means is a simple algorithm. It can be seen in pseudo-python below. Don't worry if this doesn't make sense to you at first. We will build each step together.\n",
    "\n",
    "- ** Step 1: ** randomly initialize k cluster **centroids**\n",
    "- for _ in **num_iterations** or when centroids stop moving:\n",
    " - ** Step 2: ** assign each data point to its closest centroid\n",
    " - ** Step 3: ** move centroid to the mean of all points assigned to it\n",
    "\n",
    "### First four steps of K-means visualized\n",
    "![caption](kmeans.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Blobs\n",
    "Sci-kit learn comes with a number of helpful functions to generate simulated data. We will use the ```make_blobs``` function to generate **10**, **2-dimensional** (two features) data points that belong to distinct clusters. \n",
    "\n",
    "Working with a small, admittedly boring, dataset will allow us to easily visualize the algorithm at each step. When you work with real data, k-means will work the same way, but you will often have many features, making it difficult to plot and visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Look at the Practice Dataset plotted below.We will test our k-means algorithm using this data. How many clusters do you think the dataset should be divided into?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = 3\n",
    "X, y = make_blobs(n_samples=10,\n",
    "                  n_features=2,\n",
    "                  centers=clusters, # true number of cluster centers\n",
    "                  cluster_std=0.25,\n",
    "                  center_box=(-5,5),\n",
    "                  random_state=20)\n",
    "\n",
    "X = X.round(1)\n",
    "\n",
    "print(\"X =\\n {}\".format(X))    \n",
    "plt.scatter(X[:,0], X[:,1], cmap=cmap, edgecolors='black')\n",
    "plt.axis('equal')\n",
    "plt.title('Practice Dataset');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Initialize Centroids\n",
    "**Centroids** are the points that will become the center of each of our clusters. There are many ways to initialize centroids in k-means.We will choose the simplest, called the **Forgy** method. To initialize the Forgy methodWe choose k random points from the dataset and start the centroids there.\n",
    "\n",
    "An example implementation of the Forgy method is below. It uses numpy's random.choice method to select k random numbers from 0 to len(num_training_samples). Setting replace=False ensures that it doesn't choose the same number twice. These numbers will serve as the indexes of the initial centroid position values.We then make use of numpy's \"fancy\" indexing to retrieve the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_centroids(X, k):\n",
    "    index = np.random.choice(len(X), size=k, replace=False) # choose a k random numbers in len(data)\n",
    "    return X[index] # return the points at those values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centroids = initialize_centroids(X, 3)\n",
    "centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "The following code plots 3 initialized centroids along with the Practice Dataset.\n",
    "\n",
    "Run the code a few times to see where the centroids are initialized. Remember, k-means works by assigning each data point to its closest centroid then moving each centroid to the mean of all points assigned to it. Is it possible thatWe will find different clusters based on different initializations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centroids = initialize_centroids(X, 3)\n",
    "plt.scatter(X[:, 0], X[:,1], cmap=cmap, edgecolors='black')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=200, marker='*');\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 2: Assign each data point to the closest centroid\n",
    "To assign points to their closest centroid,We need to define what \"closest\" means. K-means is typically implemented using **Euclidean Distance** (otherwise known as the distance formula you likely learned and forgot in middle school). HoIver, it is acceptable to use a different distance metric that you feel would work best for your data (e.g., **Manhattan Distance**). Examples of both Euclidean and Manhattan distance are shown below. For the rest of the mission,We will focus on Euclidean distance, but remember that your choice of distance metric will affect your results.\n",
    "\n",
    "# TODO: show how non-scaled data can \"confuse\" distance metrics\n",
    "\n",
    "![caption](distance_metrics.svg)\n",
    "- you will often see Euclidean distance written as $\\sqrt{\\sum_{i=1}^n (x_i-y_i)^2}$ and Manhattan Distance written as $\\sum_{i=1}^n |x_i-y_i|$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Use numpy to compute the euclidean distance betIen all data points and the centroid  at [1.0, 3.4].\n",
    "\n",
    "1. subtract the (1, 2) dimensional centroid from (10, 2) dimensional **X** matrix. Numpy broadcasting will make this work. \n",
    "2. take the sum of each element (axis=1), output should be (10, ) if you specify the wrong axis or forget to specify an axis, dimensions will be wrong.\n",
    "3. Square each result\n",
    "4. Take the square root of each element\n",
    "![caption](distance1.svg)\n",
    "![caption](distance2.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centroid = np.array([[1.0,  3.4]])\n",
    "\n",
    "def euclidean_distance(X, centroid):\n",
    "    \"\"\"\n",
    "    implement vectorized euclidean distance\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hint (manhattan implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def manhattan_distance(X, centroid):\n",
    "    \"\"\"\n",
    "    Vectorized manhattan distance. Computes distance betIen all data points and one cluster centroid\n",
    "    \"\"\"\n",
    "    return np.sum(abs(X - centroid), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "manhattan_distance(X, centroid), manhattan_distance(X, centroid).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AnsIr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(X, centroid):\n",
    "    return np.sqrt(np.sum(X - centroid, axis=1) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute distances betIen X and each centroid\n",
    "To find the closest centroid for each pointWe must scale our newly written distance function to work with any number of centroids. There is a fancy (and faster, and arcane) way to do this with numpy broadcasting, but a simple **for loop** over each centroid will suffice for our purposes.\n",
    "![caption](distance_matrix.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centroids = np.array([[1.0,  3.4],\n",
    "                      [ 1.1 ,  3.7],\n",
    "                      [ -4.7,  2.3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find the distance betIen each cluster centroid\n",
    "def compute_distances(X, centroids):\n",
    "    \"\"\"\n",
    "    compute distances for each centroid. output will be a matrix dimensions (num data points, num centroids)\n",
    "    \n",
    "    \"\"\"\n",
    "    distances = np.zeros((len(X), len(centroids))) # build empty (10, 3) numpy array\n",
    "    for idx, centroid in enumerate(centroids):\n",
    "        distances[:, idx] = euclidean_distance(X, centroid)\n",
    "    return distances    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distances = compute_distances(X, centroids)\n",
    "distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions \n",
    "Now thatWe have a matrix of distances, each point's label will simply be the column index of the minimum distance value. Use numpy's argmin function on the distance matrix to assign points to centroids.\n",
    "![caption](argmin.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_points(distances):\n",
    "    \"\"\"\n",
    "    assign each point to the closest centroid using argmin\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AnsIr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_points(distances):\n",
    "    \"\"\"\n",
    "    assign each point to the closest centroid using argmin\n",
    "    \"\"\"\n",
    "    return np.argmin(distances, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = assign_points(distances)\n",
    "labels, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Move Centroids\n",
    "Now thatWe have assigned points to centroids,We need to move each centroid using the mean value of all of the points assigned to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Write the code to update the position of our three centroids.\n",
    "\n",
    "For each centroid:\n",
    "- calculate the mean of all the points assigned to it\n",
    "- set it's value to the mean.\n",
    "\n",
    "This will move the centroids. \n",
    "\n",
    "## HintWe want to take the mean of all the **points** not features this time, axis=0\n",
    "We can find all points assigned to a centroid with numpy indexing:\n",
    "```python\n",
    "c0_points = X[labels==0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def move_centroids(X, labels, centroids):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def move_centroids(X, labels, centroids):\n",
    "    return np.array([X[labels==k].mean(axis=0) for k in range(len(centroids))])     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test one loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_centroids = move_centroids(X, labels, centroids)\n",
    "\n",
    "# assign again\n",
    "dist2 = compute_distances(X, new_centroids)\n",
    "labels2 = assign_points(dist2)\n",
    "labels, labels2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "At this pointWe could loop steps 2 (assign points to centroids) and 3 (relocate centroids), run them 50 times, and find clusters. HoIver,We would have no idea how Ill the clustersWe found represent our data.\n",
    "\n",
    "To evaluate how Ill our cluster centroids explain the data,We can calculate the within cluster sum of squared error or **SSE** also known as **Scatter** or **Inertia**.We find scatter to be the most literal name of the bunch, soWe will use it from here (sci-kit learn chose to use Inertia). Scatter tells us how far, in general, our points are from a cluster center. If all of the points in a cluster are very close to their cluster centroids, the scatter will be low. If there are many points far away from any cluster centroid, the scatter will be high.\n",
    "\n",
    "# - TODO: Add plots of high scatter and low scatter clusters\n",
    "\n",
    "To calculate Scatter:\n",
    "\n",
    "for each point:\n",
    "- find the distance to its nearest centroid (this will be the centroid k-means assigned to it)\n",
    "- square each distance\n",
    "- sum all the distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_scatter(X, centroids):\n",
    "    \"\"\"\n",
    "    compute the within cluster scatter\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ansIr\n",
    "def compute_scatter(X, centroids):\n",
    "    all_dists = compute_distances(centroids, X)\n",
    "    dist_to_closest_centroid = np.min(all_dists, axis=0)\n",
    "    sse = sum(dist_to_closest_centroid ** 2)\n",
    "    return sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing it all together - K-means functionWe have built all the components of k-means, nowWe can refer back to the initial pseudo-code workflow and put everything together. \n",
    "\n",
    "Remember k-means was comprised of 3 steps, two of whichWe repeat\n",
    "\n",
    "- ** Step 1: ** randomly initialize k cluster centroids\n",
    "- for _ in **num_iterations** or when centroids stop moving:\n",
    " - ** Step 2: ** assign each data point to its closest centroid\n",
    " - ** Step 3: ** move centroid to the mean of all points assigned to it\n",
    " We can add our evaluation metric on as a fourth step and translate this to code line for line\n",
    "- ** Step 4: ** compute scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def k_means(X, k, num_iter):\n",
    "    centroids = initialize_centroids(X, k) # step one\n",
    "    for _ in range(num_iter):\n",
    "        distances = compute_distances(X, centroids) # intermediate step can compute distances any way you like\n",
    "        labels = assign_points(distances) # step two\n",
    "        centroids = move_centroids(X, labels, centroids) # step 3\n",
    "    \n",
    "    scatter =  compute_scatter(X, centroids)  # compute scatter\n",
    "    return labels, centroids, scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can than run our hand built k_means function on our practice dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 3 # number of cluster centroids to initialize\n",
    "num_iter = 100 # number of iterations to make\n",
    "labels, centroids, scatter = k_means(X, k, num_iter)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and plot the results. DidWe find reasonable clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:,1], c=labels, cmap=cmap, edgecolors='black')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=200, marker='*');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "1. Run the following code with k=3 at least 10 times. Does it always give good results? If it gives bad results, can you guess why? What happens to the scatter when the results are bad?\n",
    "\n",
    "2. Run the following code with different values of k, what happens to the value of scatter as you increase k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@interact_manual(k=(1, 6))\n",
    "def plot_k_means(k):\n",
    "    # make data\n",
    "    clusters = 3\n",
    "    X, y = make_blobs(n_samples=10,\n",
    "                      n_features=2,\n",
    "                      centers=clusters, # true number of cluster centers\n",
    "                      cluster_std=0.25,\n",
    "                      center_box=(-5,5),\n",
    "                      random_state=20)\n",
    "\n",
    "    X = X.round(1)\n",
    "    \n",
    "    labels, centroids, scatter = k_means(X, k, 500) # fit our k-means\n",
    "    kmeans = KMeans(n_clusters=k).fit(X) # fit sklearn k-means\n",
    "    \n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.subplot(131)\n",
    "    plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.Pastel2, edgecolors='black')\n",
    "    plt.axis('equal')\n",
    "    plt.title(\"True Clusters = {}\".format(clusters));\n",
    "\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.scatter(X[:,0], X[:,1], c=labels, cmap=plt.cm.Pastel2, edgecolors='black')\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=200, marker='*')  \n",
    "    plt.axis('equal')\n",
    "    plt.title(\"K-Means K = {}\\n Scatter = {}\".format(k, scatter));\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.scatter(X[:,0], X[:,1], c=kmeans.labels_, cmap=plt.cm.Pastel2, edgecolors='black')\n",
    "    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='black', s=200, marker='*')\n",
    "    plt.axis('equal')\n",
    "    plt.title(\"sklearn K-Means K = {}\\n Scatter = {}\".format(k, kmeans.inertia_));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn\n",
    "If you played with the example above, you may have noticed that our implementation doesn't always give the same results as the sklearn version. In fact, sometimes it finds \"very bad\" clusters, especially at values of k > 5. This is what happens when kmeans gets stuck in a local minimum. It's usually due to an unlucky centroid initialization and can be overcome by running the algorithm many times and choosing the loIst scatter value or by using a better initialization. \n",
    "\n",
    "The k-meansWe wrote is good for learning purposes but not good enough for practice. In addition to being highly optimized, the sklearn version of kmeans uses a more sophisticated centroid initialization method called **kmeans++** which solves many of the problems the classic initialization runs into by making sure centroids aren't initialized too close together. Rather than try to reinvent the wheel,We will switch to the sklearn version from now on.\n",
    "\n",
    "Notice how our example code breaks down when given more than 3 clusters. This is due to the simple initialization methodWe used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@interact_manual(k=(1, 10), clusters=(1, 10))\n",
    "def plot_k_means(k, clusters):\n",
    "    X, y = make_blobs(n_samples=250,\n",
    "                  n_features=2,\n",
    "                  centers=clusters, # true number of cluster centers\n",
    "                  cluster_std=0.25,\n",
    "                  center_box=(-5,5),\n",
    "                  random_state=20)\n",
    "    \n",
    "    \n",
    "    labels, centroids, sse = k_means(X, k, 500) # fit our k-means\n",
    "    kmeans = KMeans(n_clusters=k).fit(X) # fit sklearn k-means\n",
    "    \n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.subplot(131)\n",
    "    plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.Pastel2, edgecolors='black')\n",
    "    plt.axis('equal')\n",
    "    plt.title(\"True Clusters = {}\".format(clusters));\n",
    "\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.scatter(X[:,0], X[:,1], c=labels, cmap=plt.cm.Pastel2, edgecolors='black')\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=200, marker='*')  \n",
    "    plt.axis('equal')\n",
    "    plt.title(\"K-Means K = {}\\n Scatter = {}\".format(k, sse));\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.scatter(X[:,0], X[:,1], c=kmeans.labels_, cmap=plt.cm.Pastel2, edgecolors='black')\n",
    "    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='black', s=200, marker='*')\n",
    "    plt.axis('equal')\n",
    "    plt.title(\"sklearn K-Means K = {}\\n Scatter = {}\".format(k, kmeans.inertia_));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Choose K\n",
    "\n",
    "One of the main downsides to k-means is that I, as users, have to choose K. In other words,We must effectively guess how many clustersWe want the algorithm to find up front. So how doWe do this? \n",
    "\n",
    "Unfortunately, there is no \"right\" way to choose K, but there are a few useful guidelines.\n",
    "\n",
    "## 1. Consider the context\n",
    "Sometimes you have a maximum number of clusters you can work with and the underlying structure of the data don't matter that much. This can crop up in **customer segmentation** and **lossy compression**. \n",
    "\n",
    "With regards to customer segmentation, if your company only has the resources (time, money) to create three different types of ads. Your task is then to segment the company's customers into no more than 3 clusters. Therefore K <= 3.\n",
    "\n",
    "With regards to compression, if you remember computer graphics in the early 90's you would often be have to choose betIen 16, 32, 64, or maybe 256 colors. Ill, 16, colors is simply an image represented with only 16 possible colors rather than 256 * 256 * 256. In this case, k = 16 was chosen because it takes less bits to represent 16 different color values than 16,777,216 different colors values. \n",
    "\n",
    "The example below uses k-means to cluster image pixels based on color features, and sets pixel color values to the values of the cluster centroid (collapsing all values to their mean). This method is formally called \"vector quantization\" andWe can learn interesting things about k-means from it. \n",
    "\n",
    "There are valuable lessons to learn from this cat picture:\n",
    "- notice that, even with very low values of kWe can still see the information the data is trying to convey (I can still see that the picture is of a cat). This is the idea behind cluster-based visualization,We reduce the number of samples but still retain as much information as possible. \n",
    "- notice that the first major color feature to disappear, at around k = 60, is the cat's pink nose. If this Ire customer data and those small number of pink pixels represented your businesses \"big spenders\" think about what would happen if they Ire absorbed into another cluster. Remember, clustering is no substitute for understanding your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat = misc.imread('small_cat.jpg')\n",
    "cat_pixels = cat.reshape(120*160, 3)\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=1337, n_jobs=-1)\n",
    "kmeans.fit(cat_pixels)\n",
    "\n",
    "def colapse_to_centroids(X, labels, centroids):\n",
    "    quantized = X.copy()\n",
    "    for c in range(len(centroids)):\n",
    "        quantized[labels==c] = centroids[c]\n",
    "    return quantized\n",
    "\n",
    "@interact_manual(k=(1, 256))\n",
    "def quantize(k):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=1337, n_jobs=4).fit(cat_pixels)\n",
    "    quantized = colapse_to_centroids(cat_pixels, kmeans.labels_, kmeans.cluster_centers_)\n",
    "    quantized_cat = quantized.reshape(120, 160, 3)\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.imshow(cat)\n",
    "    plt.title('original cat burrito - 16,777,216 possible colors')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.imshow(quantized_cat)\n",
    "    plt.title('{} color cat burrito'.format(k))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Elbow method\n",
    "If there are no external forces driving our selection of k. There are some heuristics that can help. One of which is the \"elbow\" method. To use the elbow methodWe will plot increasing values of K against their scatter values. Optimally, the graph will show a pronounced \"kink\" or \"elbow\" where the scatter(distortion, inertia) value does not decrease very much asWe increase k.We choose the value of k at this \"elbow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Elbow method:\n",
    "you can find the scatter of an sklearn model with:\n",
    "```python \n",
    "kmeans.inertia_\n",
    "```\n",
    "\n",
    "for values of k betIen 1 and 10:\n",
    "- run k-means on the toy dataset **X**\n",
    "- Scatter values corresponding to each k\n",
    "\n",
    "Make a plot with the values of k on the x axis and Scatter values on the y-axis. \n",
    "\n",
    "Use the plot to choose the k. When using real data, remember that the value of k will not always be so obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = 5\n",
    "\n",
    "X, y = make_blobs(n_samples=250,\n",
    "                  n_features=2,\n",
    "                  centers=clusters, # true number of cluster centers\n",
    "                  cluster_std=0.25,\n",
    "                  center_box=(-5,5),\n",
    "                  random_state=20)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=cmap, edgecolors='black')\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ks = []\n",
    "scatter = []\n",
    "\n",
    "for k in range(10):\n",
    "    kmeans = KMeans(n_clusters=k+1).fit(X)\n",
    "    ks.append(k+1)\n",
    "    scatter.append(kmeans.inertia_)\n",
    "    \n",
    "plt.plot(ks, scatter)\n",
    "plt.title(\"Elbow method\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Scatter\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When k-means breaks\n",
    "- TODO: circle data, swiss roll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster similar movies with k-means\n",
    "data source: https://www.kaggle.com/deepmatrix/imdb-5000-movie-dataset\n",
    "\n",
    "28 variables for 5043 movies scraped from IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movies = pd.read_csv('movie_metadata.csv')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "movies.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Keanu Reeves movies along 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = movies[movies['actor_1_name'] == 'Keanu Reeves'] # choose only movies with Keanu Reeves as the main actor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning: methods that rely on euclidean distance will not work Ill unless the data are standardized\n",
    "(hopefully this was taught in linear regression course)?\n",
    "- usually subtract mean then divide by std\n",
    "- since I're only using two variables here,We chose to divide gross by 100 million to retain its meaning in the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del X # reset df on ipython\n",
    "X = data[['imdb_score', 'gross']] #We chose to cluster movies by imdb_score and box office gross\n",
    "X = X.dropna()\n",
    "X['gross'] /= 100000000 # semi-standardization\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@interact_manual(k=(1, 6))\n",
    "def imdb(k):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=1337, n_jobs=4).fit(X)\n",
    "    \n",
    "    plt.scatter(X[:,0], X[:,1], c=kmeans.labels_, cmap=cmap, edgecolors='black')\n",
    "    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='black', s=200, marker='*')\n",
    "    plt.axis('equal')\n",
    "    plt.xlabel('imbd_score')\n",
    "    plt.ylabel('box office gross, 100 millions')\n",
    "    plt.title(\"Keanu Reeves movies K = {}\".format(k));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ks = []\n",
    "scatter = []\n",
    "\n",
    "for k in range(10):\n",
    "    kmeans = KMeans(n_clusters=k+1).fit(X)\n",
    "    ks.append(k+1)\n",
    "    scatter.append(kmeans.inertia_)\n",
    "    \n",
    "plt.plot(ks, scatter)\n",
    "plt.title(\"Elbow method\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Scatter\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "If you're into Keanu Reeves movies, it could be fun to look at how his movies fall into each cluster.We can then try to come up with a name for each cluster that explains the combination of featuresWe used in the clustering.We can already see that Keanu Reeve's highest grossing, highest rated film which gets a cluster all to itself is \"The Matrix\". But what about the other clusters? What ifWe used more features with our clustering? Would such a clustering be useful in a movie recommendation system?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=1337, n_jobs=4).fit(X)\n",
    "np.where(kmeans.labels_ == 2)\n",
    "data.iloc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "## Instructions:\n",
    "Cluster movies from the imdb5000 dataset using your choice of features. You can choose as many features as you like, but remember, if you choose more than 3 features you will not be able to visualize the clusters (yet). Use the elbow method to choose k. Have fun with it, and see if you can you come up with informative names for your clusters.\n",
    "\n",
    "\n",
    "To easily scale your features, use: \n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit_transform(X)\n",
    "```\n",
    "\n",
    "If you choose to use categorical features or text you will have to encode them appropriately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this missionWe learned about **Unsupervised Learning** and **Clustering**.We built the k-means algorithm in numpy and learned that k-means is fast, scalable and simple. HoIver, k-means doesn't always get the right ansIr, either due to poor initializations or bad distance metrics.We also learned how to use scatter (kmeans.inertia_) to evaluate how far our points are from their assigned cluster center and leveraged scatter via the elbow method to inform our choice of k. Finally,We used k-means to find groups of similar movies in the real-world **imdb5000** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_s_curve\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# X, y = make_s_curve(n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.view_init(7, -80)\n",
    "# ax.scatter(X[:,0], X[:,1], X[:,2], c=y, cmap=cmap, edgecolors='black');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make data\n",
    "# X, y = make_blobs(n_samples=250,\n",
    "#                   n_features=2,\n",
    "#                   centers=3, # true number of cluster centers\n",
    "#                   cluster_std=0.25,\n",
    "#                   center_box=(-5,5),\n",
    "#                  random_state=20)\n",
    "\n",
    "# centroids = initialize_centroids(X, 3)\n",
    "# distances = compute_distances(X, centroids)\n",
    "# labels = assign_points(distances)\n",
    "\n",
    "# new_centroids = move_centroids(X, labels, centroids)\n",
    "# dist2 = compute_distances(X, new_centroids)\n",
    "# labels2 = assign_points(dist2)\n",
    "# plt.figure(figsize=(20, 5))\n",
    "\n",
    "# plt.subplot(141)\n",
    "# plt.scatter(X[:,0], X[:,1], cmap=cmap, edgecolors='black')\n",
    "# plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=200, marker='*')\n",
    "# plt.title('Initialize k Centroids')\n",
    "# plt.axis('equal')\n",
    "\n",
    "\n",
    "# plt.subplot(142)\n",
    "# plt.scatter(X[:,0], X[:,1], c=labels, cmap=cmap, edgecolors='black')\n",
    "# plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=200, marker='*');\n",
    "# plt.title('Assign Points')\n",
    "# plt.axis('equal')\n",
    "\n",
    "\n",
    "\n",
    "# plt.subplot(143)\n",
    "# plt.scatter(X[:,0], X[:,1], c=labels, cmap=cmap, edgecolors='black')\n",
    "# plt.scatter(new_centroids[:, 0], new_centroids[:, 1], c='black', s=200, marker='*')\n",
    "# plt.title('Move Centriods')\n",
    "# plt.axis('equal')\n",
    "\n",
    "\n",
    "# plt.subplot(144)\n",
    "# plt.scatter(X[:,0], X[:,1], c=labels2, cmap=cmap, edgecolors='black')\n",
    "# plt.scatter(new_centroids[:, 0], new_centroids[:, 1], c='black', s=200, marker='*')\n",
    "# plt.title('Assign Points')\n",
    "# plt.axis('equal');\n",
    "\n",
    "\n",
    "\n",
    "# plt.savefig('kmeans.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clusters = 3\n",
    "# k = 3\n",
    "# num_iterations = 5\n",
    "\n",
    "# # make data\n",
    "# X, y = make_blobs(n_samples=250,\n",
    "#                   n_features=2,\n",
    "#                   centers=clusters, # true number of cluster centers\n",
    "#                   cluster_std=0.25,\n",
    "#                   center_box=(-5,5),\n",
    "#                   random_state=20)\n",
    "\n",
    "# centroids = initialize_centroids(X, k)\n",
    "# iteration = 1 # for looping with animate\n",
    "\n",
    "\n",
    "# fig = plt.figure();\n",
    "# ax = plt.axes()\n",
    "# line, = ax.plot([], [], lw=2)\n",
    "\n",
    "\n",
    "# def init():\n",
    "#     line.set_data([], [])\n",
    "#     return line,\n",
    "\n",
    "# def animate(i):\n",
    "#     global centroids\n",
    "#     global iteration\n",
    "#     distances = compute_distances(X, centroids)\n",
    "#     labels = assign_points(distances)\n",
    "#     centroids = move_centroids(X, labels, centroids)\n",
    "#     ax.cla()\n",
    "#     ax.scatter(X[:,0], X[:,1], c=labels, cmap=cmap, edgecolors='black')\n",
    "#     ax.scatter(centroids[:, 0], centroids[:, 1], c='black', s=200, marker='*');\n",
    "#     ax.set_title(\"K-means, {} Clusters, Iteration: {}\".format(k, iteration))\n",
    "#     ax.axis('equal')\n",
    "#     iteration += 1\n",
    "#     return line,\n",
    "\n",
    "# anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "#                         frames=num_iterations, interval=800, blit=True)\n",
    "# anim.save('kmeans.gif', writer='imagemagick', fps=1.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X = np.random.uniform(size=(150,2)) # random uniform data\n",
    "\n",
    "\n",
    "# fig = plt.figure();\n",
    "# ax = plt.axes()\n",
    "# line, = ax.plot([], [], lw=2)\n",
    "# centroids = initialize_centroids(X, 3)\n",
    "# iteration = 1\n",
    "\n",
    "# def init():\n",
    "#     line.set_data([], [])\n",
    "#     return line,\n",
    "\n",
    "# def animate(i):\n",
    "#     global centroids\n",
    "#     global iteration\n",
    "#     distances = compute_distances(X, centroids)\n",
    "#     labels = assign_points(distances)\n",
    "#     centroids = move_centroids(X, labels, centroids)\n",
    "#     ax.cla()\n",
    "#     ax.scatter(X[:,0], X[:,1], c=labels, cmap=cmap, edgecolors='black')\n",
    "#     ax.scatter(centroids[:, 0], centroids[:, 1], c='black', s=200, marker='*');\n",
    "#     ax.set_title(\"K-means Iteration: {}\".format(iteration))\n",
    "#     ax.axis('equal')\n",
    "#     iteration += 1\n",
    "#     return line,\n",
    "\n",
    "# animation.FuncAnimation(fig, animate, init_func=init,\n",
    "#                         frames=10, interval=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "- https://www.naftaliharris.com/blog/visualizing-k-means-clustering/\n",
    "- http://flothesof.github.io/k-means-numpy.html\n",
    "- http://jakevdp.github.io/blog/2012/08/18/matplotlib-animation-tutorial/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {
    "02baeafb34d2472fbd701a338cf98d0e": {
     "views": []
    },
    "07b3def2e80941afa6866f73fa9a1eaf": {
     "views": []
    },
    "0cc7206b058d4c17a42a49b97de3f3d7": {
     "views": []
    },
    "19ad019ad69044b4af5c2412f5dce4e6": {
     "views": []
    },
    "1b1da9101d6d4227ae29988edda9d0e1": {
     "views": []
    },
    "25e5ad3b37314783936a3c6b1fccc9d5": {
     "views": []
    },
    "3d1d2cc0fc6842298dc9222f7f173a70": {
     "views": []
    },
    "481f145d353b49c6a3b9098b2972ed50": {
     "views": []
    },
    "5b7c0edcf0ab44ba99a745ab103aa8ec": {
     "views": []
    },
    "6f6a31d87fac4bf89327e2ae900b26cc": {
     "views": []
    },
    "779242b960784345876dffb44b891635": {
     "views": []
    },
    "8106619145b24b0daeda4d0fc6362ee7": {
     "views": []
    },
    "847c655965eb4ce28d40bb4846fd13a6": {
     "views": []
    },
    "8abfec20d1a7441988d9c0e15296aa2f": {
     "views": []
    },
    "90112b5b23ca4c79945f696711eca1c6": {
     "views": []
    },
    "a340018f89824c81a4321f397370412d": {
     "views": []
    },
    "ae3acbcd01c642a98fe166e8dbcd0152": {
     "views": []
    },
    "afa05e1b86564027b608ee986d3a5147": {
     "views": []
    },
    "b63835106f55435b8082aa9f3e0d7edf": {
     "views": []
    },
    "c0f8262a811f4f4d8bf4bdfa92aa090c": {
     "views": []
    },
    "d63a4d5a285145e2bee623540ecef70d": {
     "views": []
    },
    "db0ba8e457bf41dca2ba5edb1ae617d6": {
     "views": []
    },
    "e794194369f04a70a9a95d2edea65325": {
     "views": []
    },
    "e9dc3547f6734877a256566c4b125177": {
     "views": []
    },
    "f9a1c52214e04a2abcaca1e82971efc2": {
     "views": []
    },
    "ff3ccae4ece94dfc8e7dd337764165aa": {
     "views": []
    }
   },
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
